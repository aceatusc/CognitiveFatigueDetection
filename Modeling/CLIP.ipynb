{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from CogBeaconDataset import CogBeaconDataset\n",
    "\n",
    "cogbeacon_root_path = '/Users/athenasaghi/VSProjects/CognitiveFatigueDetection/CogFatigueData/CogBeacon/'\n",
    "dataset = CogBeaconDataset(cogbeacon_root_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50, Contrastive Loss: 1.0696\n",
      "Epoch 2/50, Contrastive Loss: 0.7878\n",
      "Epoch 3/50, Contrastive Loss: 0.6987\n",
      "Epoch 4/50, Contrastive Loss: 0.7251\n",
      "Epoch 5/50, Contrastive Loss: 0.6157\n",
      "Epoch 6/50, Contrastive Loss: 0.6419\n",
      "Epoch 7/50, Contrastive Loss: 0.6499\n",
      "Epoch 8/50, Contrastive Loss: 0.6635\n",
      "Epoch 9/50, Contrastive Loss: 0.6845\n",
      "Epoch 10/50, Contrastive Loss: 0.6212\n",
      "Epoch 11/50, Contrastive Loss: 0.6332\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 92\u001b[0m\n\u001b[1;32m     90\u001b[0m     z_f \u001b[38;5;241m=\u001b[39m encoder_features(x_f_batch, training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     91\u001b[0m     loss \u001b[38;5;241m=\u001b[39m contrastive_loss(z_r, z_f)\n\u001b[0;32m---> 92\u001b[0m grads \u001b[38;5;241m=\u001b[39m tape\u001b[38;5;241m.\u001b[39mgradient(loss, encoder_raweeg\u001b[38;5;241m.\u001b[39mtrainable_variables \u001b[38;5;241m+\u001b[39m encoder_features\u001b[38;5;241m.\u001b[39mtrainable_variables)\n\u001b[1;32m     93\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mapply_gradients(\u001b[38;5;28mzip\u001b[39m(grads, encoder_raweeg\u001b[38;5;241m.\u001b[39mtrainable_variables \u001b[38;5;241m+\u001b[39m encoder_features\u001b[38;5;241m.\u001b[39mtrainable_variables))\n\u001b[1;32m     94\u001b[0m losses\u001b[38;5;241m.\u001b[39mappend(loss\u001b[38;5;241m.\u001b[39mnumpy())\n",
      "File \u001b[0;32m~/anaconda3/envs/CognitiveFatigue/lib/python3.11/site-packages/tensorflow/python/eager/backprop.py:1066\u001b[0m, in \u001b[0;36mGradientTape.gradient\u001b[0;34m(self, target, sources, output_gradients, unconnected_gradients)\u001b[0m\n\u001b[1;32m   1060\u001b[0m   output_gradients \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   1061\u001b[0m       composite_tensor_gradient\u001b[38;5;241m.\u001b[39mget_flat_tensors_for_gradients(\n\u001b[1;32m   1062\u001b[0m           output_gradients))\n\u001b[1;32m   1063\u001b[0m   output_gradients \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m x \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m ops\u001b[38;5;241m.\u001b[39mconvert_to_tensor(x)\n\u001b[1;32m   1064\u001b[0m                       \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m output_gradients]\n\u001b[0;32m-> 1066\u001b[0m flat_grad \u001b[38;5;241m=\u001b[39m imperative_grad\u001b[38;5;241m.\u001b[39mimperative_grad(\n\u001b[1;32m   1067\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tape,\n\u001b[1;32m   1068\u001b[0m     flat_targets,\n\u001b[1;32m   1069\u001b[0m     flat_sources,\n\u001b[1;32m   1070\u001b[0m     output_gradients\u001b[38;5;241m=\u001b[39moutput_gradients,\n\u001b[1;32m   1071\u001b[0m     sources_raw\u001b[38;5;241m=\u001b[39mflat_sources_raw,\n\u001b[1;32m   1072\u001b[0m     unconnected_gradients\u001b[38;5;241m=\u001b[39munconnected_gradients)\n\u001b[1;32m   1074\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_persistent:\n\u001b[1;32m   1075\u001b[0m   \u001b[38;5;66;03m# Keep track of watched variables before setting tape to None\u001b[39;00m\n\u001b[1;32m   1076\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_watched_variables \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tape\u001b[38;5;241m.\u001b[39mwatched_variables()\n",
      "File \u001b[0;32m~/anaconda3/envs/CognitiveFatigue/lib/python3.11/site-packages/tensorflow/python/eager/imperative_grad.py:67\u001b[0m, in \u001b[0;36mimperative_grad\u001b[0;34m(tape, target, sources, output_gradients, sources_raw, unconnected_gradients)\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m:\n\u001b[1;32m     64\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m     65\u001b[0m       \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnknown value for unconnected_gradients: \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m unconnected_gradients)\n\u001b[0;32m---> 67\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m pywrap_tfe\u001b[38;5;241m.\u001b[39mTFE_Py_TapeGradient(\n\u001b[1;32m     68\u001b[0m     tape\u001b[38;5;241m.\u001b[39m_tape,  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     target,\n\u001b[1;32m     70\u001b[0m     sources,\n\u001b[1;32m     71\u001b[0m     output_gradients,\n\u001b[1;32m     72\u001b[0m     sources_raw,\n\u001b[1;32m     73\u001b[0m     compat\u001b[38;5;241m.\u001b[39mas_str(unconnected_gradients\u001b[38;5;241m.\u001b[39mvalue))\n",
      "File \u001b[0;32m~/anaconda3/envs/CognitiveFatigue/lib/python3.11/site-packages/tensorflow/python/eager/backprop.py:148\u001b[0m, in \u001b[0;36m_gradient_function\u001b[0;34m(op_name, attr_tuple, num_inputs, inputs, outputs, out_grads, skip_input_indices, forward_pass_name_scope)\u001b[0m\n\u001b[1;32m    146\u001b[0m     gradient_name_scope \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m forward_pass_name_scope \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    147\u001b[0m   \u001b[38;5;28;01mwith\u001b[39;00m ops\u001b[38;5;241m.\u001b[39mname_scope(gradient_name_scope):\n\u001b[0;32m--> 148\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m grad_fn(mock_op, \u001b[38;5;241m*\u001b[39mout_grads)\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m grad_fn(mock_op, \u001b[38;5;241m*\u001b[39mout_grads)\n",
      "File \u001b[0;32m~/anaconda3/envs/CognitiveFatigue/lib/python3.11/site-packages/tensorflow/python/ops/linalg_grad.py:344\u001b[0m, in \u001b[0;36m_EinsumGrad\u001b[0;34m(op, grad)\u001b[0m\n\u001b[1;32m    342\u001b[0m y_shape \u001b[38;5;241m=\u001b[39m array_ops\u001b[38;5;241m.\u001b[39mshape(y)\n\u001b[1;32m    343\u001b[0m grad_x \u001b[38;5;241m=\u001b[39m _GetGradWrt(grad, y, x_shape, x_subs, y_subs, output_subs)\n\u001b[0;32m--> 344\u001b[0m grad_y \u001b[38;5;241m=\u001b[39m _GetGradWrt(grad, x, y_shape, y_subs, x_subs, output_subs)\n\u001b[1;32m    346\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ellipsis \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m output_subs:\n\u001b[1;32m    347\u001b[0m   \u001b[38;5;66;03m# If no ellipsis in the output; then no need to unbroadcast.\u001b[39;00m\n\u001b[1;32m    348\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m grad_x, grad_y\n",
      "File \u001b[0;32m~/anaconda3/envs/CognitiveFatigue/lib/python3.11/site-packages/tensorflow/python/ops/linalg_grad.py:284\u001b[0m, in \u001b[0;36m_EinsumGrad.<locals>._GetGradWrt\u001b[0;34m(output_grad, other_operand, input_shape, input_subs, other_subs, output_subs)\u001b[0m\n\u001b[1;32m    280\u001b[0m left_subs \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(s \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m input_subs \u001b[38;5;28;01mif\u001b[39;00m s \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m reduced_label_set)\n\u001b[1;32m    282\u001b[0m \u001b[38;5;66;03m# Compute the gradient wrt the input, without accounting for the operation\u001b[39;00m\n\u001b[1;32m    283\u001b[0m \u001b[38;5;66;03m# \"abc->ac\". So, now we have the VJP of the operation \"ac,cd->ad\".\u001b[39;00m\n\u001b[0;32m--> 284\u001b[0m grad_reduced \u001b[38;5;241m=\u001b[39m gen_linalg_ops\u001b[38;5;241m.\u001b[39meinsum([output_grad, other_operand],\n\u001b[1;32m    285\u001b[0m                                      \u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m,\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m->\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m    286\u001b[0m                                          output_subs, other_subs,\n\u001b[1;32m    287\u001b[0m                                          left_subs))\n\u001b[1;32m    288\u001b[0m \u001b[38;5;66;03m# If the reduced_label_set is empty, then we already have the gradient\u001b[39;00m\n\u001b[1;32m    289\u001b[0m \u001b[38;5;66;03m# wrt the input.\u001b[39;00m\n\u001b[1;32m    290\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m reduced_label_set:\n",
      "File \u001b[0;32m~/anaconda3/envs/CognitiveFatigue/lib/python3.11/site-packages/tensorflow/python/ops/gen_linalg_ops.py:1115\u001b[0m, in \u001b[0;36meinsum\u001b[0;34m(inputs, equation, name)\u001b[0m\n\u001b[1;32m   1113\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tld\u001b[38;5;241m.\u001b[39mis_eager:\n\u001b[1;32m   1114\u001b[0m   \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1115\u001b[0m     _result \u001b[38;5;241m=\u001b[39m pywrap_tfe\u001b[38;5;241m.\u001b[39mTFE_Py_FastPathExecute(\n\u001b[1;32m   1116\u001b[0m       _ctx, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEinsum\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, inputs, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mequation\u001b[39m\u001b[38;5;124m\"\u001b[39m, equation)\n\u001b[1;32m   1117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _result\n\u001b[1;32m   1118\u001b[0m   \u001b[38;5;28;01mexcept\u001b[39;00m _core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Dense, Flatten, Concatenate, BatchNormalization, Dropout, MultiHeadAttention, LayerNormalization, Add\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.utils import shuffle, resample\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load Data\n",
    "df = pd.read_pickle('processed_data_full.pkl')\n",
    "X_raweeg = np.stack(df['raweeg'].values)\n",
    "X_features = np.stack(df['features'].values)\n",
    "Y_labels = df['label'].values\n",
    "\n",
    "# Remove class 3\n",
    "mask = Y_labels != 3\n",
    "X_raweeg = X_raweeg[mask]\n",
    "X_features = X_features[mask]\n",
    "Y_labels = Y_labels[mask]\n",
    "\n",
    "num_classes = len(np.unique(Y_labels))\n",
    "Y_labels = to_categorical(Y_labels, num_classes=num_classes)\n",
    "\n",
    "# Split data first\n",
    "X_train_raweeg, X_test_raweeg, X_train_features, X_test_features, Y_train, Y_test = train_test_split(\n",
    "    X_raweeg, X_features, Y_labels, test_size=0.1, random_state=42, stratify=np.argmax(Y_labels, axis=1)\n",
    ")\n",
    "\n",
    "# Standardize Data (only on training set)\n",
    "scaler = StandardScaler()\n",
    "X_train_raweeg = scaler.fit_transform(X_train_raweeg.reshape(-1, X_train_raweeg.shape[-1])).reshape(X_train_raweeg.shape)\n",
    "X_test_raweeg = scaler.transform(X_test_raweeg.reshape(-1, X_test_raweeg.shape[-1])).reshape(X_test_raweeg.shape)\n",
    "X_train_features = scaler.fit_transform(X_train_features.reshape(-1, X_train_features.shape[-1])).reshape(X_train_features.shape)\n",
    "X_test_features = scaler.transform(X_test_features.reshape(-1, X_test_features.shape[-1])).reshape(X_test_features.shape)\n",
    "\n",
    "# Oversample Minority Classes with Gaussian Noise\n",
    "X_resampled, Y_resampled, Xf_resampled = [], [], []\n",
    "labels = np.argmax(Y_train, axis=1)\n",
    "for cls in np.unique(labels):\n",
    "    cls_indices = np.where(labels == cls)[0]\n",
    "    X_cls_raweeg, X_cls_features, Y_cls = resample(\n",
    "        X_train_raweeg[cls_indices], X_train_features[cls_indices], Y_train[cls_indices],\n",
    "        n_samples=max([len(np.where(labels == c)[0]) for c in np.unique(labels)]),\n",
    "        random_state=42\n",
    "    )\n",
    "    # Add small noise to avoid overfitting to duplicates\n",
    "    X_cls_raweeg += np.random.normal(0, 0.01, X_cls_raweeg.shape)\n",
    "    X_cls_features += np.random.normal(0, 0.01, X_cls_features.shape)\n",
    "    X_resampled.append(X_cls_raweeg)\n",
    "    Y_resampled.append(Y_cls)\n",
    "    Xf_resampled.append(X_cls_features)\n",
    "\n",
    "X_train_raweeg = np.vstack(X_resampled)\n",
    "Y_train = np.vstack(Y_resampled)\n",
    "X_train_features = np.vstack(Xf_resampled)\n",
    "\n",
    "# Shuffle Dataset\n",
    "X_train_raweeg, X_train_features, Y_train = shuffle(X_train_raweeg, X_train_features, Y_train, random_state=42)\n",
    "\n",
    "# Define CLIP-style contrastive loss\n",
    "def contrastive_loss(z_raweeg, z_features, temperature=0.07):\n",
    "    z_raweeg = tf.math.l2_normalize(z_raweeg, axis=1)\n",
    "    z_features = tf.math.l2_normalize(z_features, axis=1)\n",
    "    logits = tf.matmul(z_raweeg, z_features, transpose_b=True) / temperature\n",
    "    labels = tf.range(tf.shape(logits)[0])\n",
    "    return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)\n",
    "\n",
    "# Pretrain using contrastive learning\n",
    "encoder_raweeg = tf.keras.models.load_model('encoder_raweeg_clip.h5', compile=False)\n",
    "encoder_features = tf.keras.models.load_model('encoder_features_clip.h5', compile=False)\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=5e-4)\n",
    "early_stopping = EarlyStopping(monitor='loss', patience=5, restore_best_weights=True)\n",
    "\n",
    "for epoch in range(50):\n",
    "    idx = np.random.permutation(len(X_train_raweeg))\n",
    "    X_train_raweeg_shuffled, X_train_features_shuffled = X_train_raweeg[idx], X_train_features[idx]\n",
    "    losses = []\n",
    "    for i in range(0, len(X_train_raweeg), 64):\n",
    "        x_r_batch = X_train_raweeg_shuffled[i:i+64]\n",
    "        x_f_batch = X_train_features_shuffled[i:i+64]\n",
    "        if x_r_batch.shape[0] != 64:\n",
    "            continue\n",
    "        with tf.GradientTape() as tape:\n",
    "            z_r = encoder_raweeg(x_r_batch, training=True)\n",
    "            z_f = encoder_features(x_f_batch, training=True)\n",
    "            loss = contrastive_loss(z_r, z_f)\n",
    "        grads = tape.gradient(loss, encoder_raweeg.trainable_variables + encoder_features.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(grads, encoder_raweeg.trainable_variables + encoder_features.trainable_variables))\n",
    "        losses.append(loss.numpy())\n",
    "    print(f\"Epoch {epoch+1}/50, Contrastive Loss: {np.mean(losses):.4f}\")\n",
    "\n",
    "# Save trained encoders\n",
    "encoder_raweeg.save('encoder_raweeg_clip_2.h5')\n",
    "encoder_features.save('encoder_features_clip_2.h5')\n",
    "\n",
    "# Continue with classification using pretrained encoders\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "for train_index, val_index in skf.split(X_train_raweeg, np.argmax(Y_train, axis=1)):\n",
    "    X_train_fold_raweeg, X_val_raweeg = X_train_raweeg[train_index], X_train_raweeg[val_index]\n",
    "    X_train_fold_features, X_val_features = X_train_features[train_index], X_train_features[val_index]\n",
    "    Y_train_fold, Y_val = Y_train[train_index], Y_train[val_index]\n",
    "    \n",
    "    input_raweeg = Input(shape=(X_train_raweeg.shape[1], X_train_raweeg.shape[2]))\n",
    "    input_features = Input(shape=(X_train_features.shape[1], X_train_features.shape[2]))\n",
    "    z_raweeg = encoder_raweeg(input_raweeg)\n",
    "    z_features = encoder_features(input_features)\n",
    "    combined = Concatenate()([z_raweeg, z_features])\n",
    "    output = Dense(num_classes, activation='softmax', kernel_regularizer=l2(1e-5))(combined)\n",
    "    classifier = Model(inputs=[input_raweeg, input_features], outputs=output)\n",
    "    classifier.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy', 'AUC'])\n",
    "    classifier.fit([X_train_fold_raweeg, X_train_fold_features], Y_train_fold, validation_data=([X_val_raweeg, X_val_features], Y_val), epochs=30, batch_size=64, callbacks=[early_stopping])\n",
    "\n",
    "classifier.save('classifier_clip.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CognitiveFatigue",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
