{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Dense, Flatten, Concatenate, BatchNormalization, Dropout, LSTM, SpatialDropout1D\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.model_selection import KFold, train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.utils import resample\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Attention, GlobalAveragePooling1D\n",
    "\n",
    "import wandb\n",
    "df = pd.read_pickle('/Users/athenasaghi/VSProjects/CognitiveFatigueDetection/processed_data_full.pkl')\n",
    "\n",
    "X_raweeg = np.stack(df['raweeg'].values)\n",
    "X_features = np.stack(df['features'].values)\n",
    "Y_labels = df['label'].values\n",
    "\n",
    "# Remove class 3\n",
    "mask = Y_labels != 3\n",
    "X_raweeg = X_raweeg[mask]\n",
    "X_features = X_features[mask]\n",
    "Y_labels = Y_labels[mask]\n",
    "\n",
    "# Update labels: 0 -> 0, 1 and 2 -> 1\n",
    "# Y_labels[Y_labels == 1] = 1\n",
    "Y_labels[Y_labels == 2] = 1\n",
    "\n",
    "num_classes = len(np.unique(Y_labels))\n",
    "Y_labels = to_categorical(Y_labels, num_classes=num_classes)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_raweeg = scaler.fit_transform(X_raweeg.reshape(-1, X_raweeg.shape[-1])).reshape(X_raweeg.shape)\n",
    "X_features = scaler.fit_transform(X_features.reshape(-1, X_features.shape[-1])).reshape(X_features.shape)\n",
    "\n",
    "# Replace padding zeros with small noise to avoid learning issues\n",
    "X_raweeg[X_raweeg == 0] = np.random.normal(loc=0, scale=1e-6, size=np.sum(X_raweeg == 0))\n",
    "\n",
    "# Oversample minority classes\n",
    "X_resampled, Y_resampled, Xf_resampled = [], [], []\n",
    "labels = np.argmax(Y_labels, axis=1)\n",
    "for cls in np.unique(labels):\n",
    "    cls_indices = np.where(labels == cls)[0]\n",
    "    X_cls_raweeg = X_raweeg[cls_indices]\n",
    "    X_cls_features = X_features[cls_indices]\n",
    "    Y_cls = Y_labels[cls_indices]\n",
    "    X_cls_raweeg, Y_cls, X_cls_features = resample(X_cls_raweeg, Y_cls, X_cls_features, n_samples=max([len(np.where(labels == c)[0]) for c in np.unique(labels)]), random_state=42)\n",
    "    X_resampled.append(X_cls_raweeg)\n",
    "    Y_resampled.append(Y_cls)\n",
    "    Xf_resampled.append(X_cls_features)\n",
    "\n",
    "X_raweeg = np.vstack(X_resampled)\n",
    "Y_labels = np.vstack(Y_resampled)\n",
    "X_features = np.vstack(Xf_resampled)\n",
    "\n",
    "# Split into train and test sets\n",
    "X_train_raweeg, X_test_raweeg, X_train_features, X_test_features, Y_train, Y_test = train_test_split(\n",
    "    X_raweeg, X_features, Y_labels, test_size=0.2, random_state=42, stratify=np.argmax(Y_labels, axis=1)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(78, 1854, 4) (78, 202, 16) (78, 2)\n",
      "(20, 1854, 4) (20, 202, 16) (20, 2)\n",
      "Epoch 1/400, Contrastive Loss: 4.2200\n",
      "Epoch 2/400, Contrastive Loss: 4.1952\n",
      "Epoch 3/400, Contrastive Loss: 4.1598\n",
      "Epoch 4/400, Contrastive Loss: 4.1591\n",
      "Epoch 5/400, Contrastive Loss: 4.1595\n",
      "Epoch 6/400, Contrastive Loss: 4.1574\n",
      "Epoch 7/400, Contrastive Loss: 4.1572\n",
      "Epoch 8/400, Contrastive Loss: 4.1565\n",
      "Epoch 9/400, Contrastive Loss: 4.1563\n",
      "Epoch 10/400, Contrastive Loss: 4.1564\n",
      "Epoch 11/400, Contrastive Loss: 4.1547\n",
      "Epoch 12/400, Contrastive Loss: 4.1550\n",
      "Epoch 13/400, Contrastive Loss: 4.1542\n",
      "Epoch 14/400, Contrastive Loss: 4.1532\n",
      "Epoch 15/400, Contrastive Loss: 4.1516\n",
      "Epoch 16/400, Contrastive Loss: 4.1469\n",
      "Epoch 17/400, Contrastive Loss: 4.1442\n",
      "Epoch 18/400, Contrastive Loss: 4.1387\n",
      "Epoch 19/400, Contrastive Loss: 4.1327\n",
      "Epoch 20/400, Contrastive Loss: 4.1172\n",
      "Epoch 21/400, Contrastive Loss: 4.1088\n",
      "Epoch 22/400, Contrastive Loss: 4.1108\n",
      "Epoch 23/400, Contrastive Loss: 4.1167\n",
      "Epoch 24/400, Contrastive Loss: 4.1082\n",
      "Epoch 25/400, Contrastive Loss: 4.0904\n",
      "Epoch 26/400, Contrastive Loss: 4.0572\n",
      "Epoch 27/400, Contrastive Loss: 4.0421\n",
      "Epoch 28/400, Contrastive Loss: 4.0205\n",
      "Epoch 29/400, Contrastive Loss: 3.9695\n",
      "Epoch 30/400, Contrastive Loss: 3.9743\n",
      "Epoch 31/400, Contrastive Loss: 3.9502\n",
      "Epoch 32/400, Contrastive Loss: 4.0022\n",
      "Epoch 33/400, Contrastive Loss: 3.8966\n",
      "Epoch 34/400, Contrastive Loss: 3.9422\n",
      "Epoch 35/400, Contrastive Loss: 3.9902\n",
      "Epoch 36/400, Contrastive Loss: 3.9379\n",
      "Epoch 37/400, Contrastive Loss: 3.8711\n",
      "Epoch 38/400, Contrastive Loss: 3.9010\n",
      "Epoch 39/400, Contrastive Loss: 3.9009\n",
      "Epoch 40/400, Contrastive Loss: 3.9260\n",
      "Epoch 41/400, Contrastive Loss: 3.8763\n",
      "Epoch 42/400, Contrastive Loss: 3.8451\n",
      "Epoch 43/400, Contrastive Loss: 3.8632\n",
      "Epoch 44/400, Contrastive Loss: 3.8474\n",
      "Epoch 45/400, Contrastive Loss: 3.8667\n",
      "Epoch 46/400, Contrastive Loss: 3.8386\n",
      "Epoch 47/400, Contrastive Loss: 3.9991\n",
      "Epoch 48/400, Contrastive Loss: 3.9565\n",
      "Epoch 49/400, Contrastive Loss: 3.9145\n",
      "Epoch 50/400, Contrastive Loss: 3.8759\n",
      "Epoch 51/400, Contrastive Loss: 3.8830\n",
      "Epoch 52/400, Contrastive Loss: 3.8341\n",
      "Epoch 53/400, Contrastive Loss: 3.8539\n",
      "Epoch 54/400, Contrastive Loss: 3.7594\n",
      "Epoch 55/400, Contrastive Loss: 3.8882\n",
      "Epoch 56/400, Contrastive Loss: 3.8444\n",
      "Epoch 57/400, Contrastive Loss: 3.9063\n",
      "Epoch 58/400, Contrastive Loss: 3.9241\n",
      "Epoch 59/400, Contrastive Loss: 3.8076\n",
      "Epoch 60/400, Contrastive Loss: 3.8254\n",
      "Epoch 61/400, Contrastive Loss: 3.7521\n",
      "Epoch 62/400, Contrastive Loss: 3.7869\n",
      "Epoch 63/400, Contrastive Loss: 3.8962\n",
      "Epoch 64/400, Contrastive Loss: 3.8829\n",
      "Epoch 65/400, Contrastive Loss: 3.7529\n",
      "Epoch 66/400, Contrastive Loss: 3.7369\n",
      "Epoch 67/400, Contrastive Loss: 3.7571\n",
      "Epoch 68/400, Contrastive Loss: 3.7055\n",
      "Epoch 69/400, Contrastive Loss: 3.8062\n",
      "Epoch 70/400, Contrastive Loss: 3.8657\n",
      "Epoch 71/400, Contrastive Loss: 3.7291\n",
      "Epoch 72/400, Contrastive Loss: 3.7810\n",
      "Epoch 73/400, Contrastive Loss: 3.7414\n",
      "Epoch 74/400, Contrastive Loss: 3.7019\n",
      "Epoch 75/400, Contrastive Loss: 3.7811\n",
      "Epoch 76/400, Contrastive Loss: 3.7846\n",
      "Epoch 77/400, Contrastive Loss: 3.7740\n",
      "Epoch 78/400, Contrastive Loss: 3.7033\n",
      "Epoch 79/400, Contrastive Loss: 3.7434\n",
      "Epoch 80/400, Contrastive Loss: 3.7699\n",
      "Epoch 81/400, Contrastive Loss: 3.6873\n",
      "Epoch 82/400, Contrastive Loss: 3.6962\n",
      "Epoch 83/400, Contrastive Loss: 3.6808\n",
      "Epoch 84/400, Contrastive Loss: 3.6780\n",
      "Epoch 85/400, Contrastive Loss: 3.7231\n",
      "Epoch 86/400, Contrastive Loss: 3.6804\n",
      "Epoch 87/400, Contrastive Loss: 3.6688\n",
      "Epoch 88/400, Contrastive Loss: 3.5875\n",
      "Epoch 89/400, Contrastive Loss: 3.7329\n",
      "Epoch 90/400, Contrastive Loss: 3.6442\n",
      "Epoch 91/400, Contrastive Loss: 3.6404\n",
      "Epoch 92/400, Contrastive Loss: 3.6423\n",
      "Epoch 93/400, Contrastive Loss: 3.6780\n",
      "Epoch 94/400, Contrastive Loss: 3.6846\n",
      "Epoch 95/400, Contrastive Loss: 3.6953\n",
      "Epoch 96/400, Contrastive Loss: 3.7163\n",
      "Epoch 97/400, Contrastive Loss: 3.6397\n",
      "Epoch 98/400, Contrastive Loss: 3.5550\n",
      "Epoch 99/400, Contrastive Loss: 3.5915\n",
      "Epoch 100/400, Contrastive Loss: 3.6180\n",
      "Epoch 101/400, Contrastive Loss: 3.5880\n",
      "Epoch 102/400, Contrastive Loss: 3.5812\n",
      "Epoch 103/400, Contrastive Loss: 3.5351\n",
      "Epoch 104/400, Contrastive Loss: 3.5641\n",
      "Epoch 105/400, Contrastive Loss: 3.6602\n",
      "Epoch 106/400, Contrastive Loss: 3.6228\n",
      "Epoch 107/400, Contrastive Loss: 3.5702\n",
      "Epoch 108/400, Contrastive Loss: 3.5647\n",
      "Epoch 109/400, Contrastive Loss: 3.5873\n",
      "Epoch 110/400, Contrastive Loss: 3.5563\n",
      "Epoch 111/400, Contrastive Loss: 3.4844\n",
      "Epoch 112/400, Contrastive Loss: 3.5442\n",
      "Epoch 113/400, Contrastive Loss: 3.5304\n",
      "Epoch 114/400, Contrastive Loss: 3.5222\n",
      "Epoch 115/400, Contrastive Loss: 3.6212\n",
      "Epoch 116/400, Contrastive Loss: 3.6013\n",
      "Epoch 117/400, Contrastive Loss: 3.4110\n",
      "Epoch 118/400, Contrastive Loss: 3.4490\n",
      "Epoch 119/400, Contrastive Loss: 3.4781\n",
      "Epoch 120/400, Contrastive Loss: 3.4836\n",
      "Epoch 121/400, Contrastive Loss: 3.4193\n",
      "Epoch 122/400, Contrastive Loss: 3.3251\n",
      "Epoch 123/400, Contrastive Loss: 3.5386\n",
      "Epoch 124/400, Contrastive Loss: 3.4272\n",
      "Epoch 125/400, Contrastive Loss: 3.4072\n",
      "Epoch 126/400, Contrastive Loss: 3.3783\n",
      "Epoch 127/400, Contrastive Loss: 3.4065\n",
      "Epoch 128/400, Contrastive Loss: 3.4051\n",
      "Epoch 129/400, Contrastive Loss: 3.4477\n",
      "Epoch 130/400, Contrastive Loss: 3.4567\n",
      "Epoch 131/400, Contrastive Loss: 3.3930\n",
      "Epoch 132/400, Contrastive Loss: 3.3312\n",
      "Epoch 133/400, Contrastive Loss: 3.4188\n",
      "Epoch 134/400, Contrastive Loss: 3.3378\n",
      "Epoch 135/400, Contrastive Loss: 3.3260\n",
      "Epoch 136/400, Contrastive Loss: 3.3930\n",
      "Epoch 137/400, Contrastive Loss: 3.3748\n",
      "Epoch 138/400, Contrastive Loss: 3.3543\n",
      "Epoch 139/400, Contrastive Loss: 3.3301\n",
      "Epoch 140/400, Contrastive Loss: 3.2848\n",
      "Epoch 141/400, Contrastive Loss: 3.3310\n",
      "Epoch 142/400, Contrastive Loss: 3.3751\n",
      "Epoch 143/400, Contrastive Loss: 3.3804\n",
      "Epoch 144/400, Contrastive Loss: 3.3900\n",
      "Epoch 145/400, Contrastive Loss: 3.2510\n",
      "Epoch 146/400, Contrastive Loss: 3.2244\n",
      "Epoch 147/400, Contrastive Loss: 3.2745\n",
      "Epoch 148/400, Contrastive Loss: 3.3384\n",
      "Epoch 149/400, Contrastive Loss: 3.2890\n",
      "Epoch 150/400, Contrastive Loss: 3.2413\n",
      "Epoch 151/400, Contrastive Loss: 3.2401\n",
      "Epoch 152/400, Contrastive Loss: 3.2463\n",
      "Epoch 153/400, Contrastive Loss: 3.4137\n",
      "Epoch 154/400, Contrastive Loss: 3.2976\n",
      "Epoch 155/400, Contrastive Loss: 3.6706\n",
      "Epoch 156/400, Contrastive Loss: 3.5358\n",
      "Epoch 157/400, Contrastive Loss: 3.3970\n",
      "Epoch 158/400, Contrastive Loss: 3.3940\n",
      "Epoch 159/400, Contrastive Loss: 3.5829\n",
      "Epoch 160/400, Contrastive Loss: 3.3559\n",
      "Epoch 161/400, Contrastive Loss: 3.6068\n",
      "Epoch 162/400, Contrastive Loss: 3.5104\n",
      "Epoch 163/400, Contrastive Loss: 3.3421\n",
      "Epoch 164/400, Contrastive Loss: 3.2676\n",
      "Epoch 165/400, Contrastive Loss: 3.3306\n",
      "Epoch 166/400, Contrastive Loss: 3.2440\n",
      "Epoch 167/400, Contrastive Loss: 3.3023\n",
      "Epoch 168/400, Contrastive Loss: 3.3253\n",
      "Epoch 169/400, Contrastive Loss: 3.2731\n",
      "Epoch 170/400, Contrastive Loss: 3.4428\n",
      "Epoch 171/400, Contrastive Loss: 3.2235\n",
      "Epoch 172/400, Contrastive Loss: 3.1894\n",
      "Epoch 173/400, Contrastive Loss: 3.2366\n",
      "Epoch 174/400, Contrastive Loss: 3.2348\n",
      "Epoch 175/400, Contrastive Loss: 3.2565\n",
      "Epoch 176/400, Contrastive Loss: 3.2618\n",
      "Epoch 177/400, Contrastive Loss: 3.2331\n",
      "Epoch 178/400, Contrastive Loss: 3.1811\n",
      "Epoch 179/400, Contrastive Loss: 3.2844\n",
      "Epoch 180/400, Contrastive Loss: 3.2437\n",
      "Epoch 181/400, Contrastive Loss: 3.2319\n",
      "Epoch 182/400, Contrastive Loss: 3.1721\n",
      "Epoch 183/400, Contrastive Loss: 3.2326\n",
      "Epoch 184/400, Contrastive Loss: 3.2237\n",
      "Epoch 185/400, Contrastive Loss: 3.3308\n",
      "Epoch 186/400, Contrastive Loss: 3.2657\n",
      "Epoch 187/400, Contrastive Loss: 3.2428\n",
      "Epoch 188/400, Contrastive Loss: 3.2303\n",
      "Epoch 189/400, Contrastive Loss: 3.2813\n",
      "Epoch 190/400, Contrastive Loss: 3.2474\n",
      "Epoch 191/400, Contrastive Loss: 3.1622\n",
      "Epoch 192/400, Contrastive Loss: 3.1780\n",
      "Epoch 193/400, Contrastive Loss: 3.2184\n",
      "Epoch 194/400, Contrastive Loss: 3.1749\n",
      "Epoch 195/400, Contrastive Loss: 3.2521\n",
      "Epoch 196/400, Contrastive Loss: 3.1879\n",
      "Epoch 197/400, Contrastive Loss: 3.0812\n",
      "Epoch 198/400, Contrastive Loss: 3.1851\n",
      "Epoch 199/400, Contrastive Loss: 3.1854\n",
      "Epoch 200/400, Contrastive Loss: 3.2098\n",
      "Epoch 201/400, Contrastive Loss: 3.1588\n",
      "Epoch 202/400, Contrastive Loss: 3.1645\n",
      "Epoch 203/400, Contrastive Loss: 3.2436\n",
      "Epoch 204/400, Contrastive Loss: 3.1943\n",
      "Epoch 205/400, Contrastive Loss: 3.1274\n",
      "Epoch 206/400, Contrastive Loss: 3.1818\n",
      "Epoch 207/400, Contrastive Loss: 3.2529\n",
      "Epoch 208/400, Contrastive Loss: 3.1716\n",
      "Epoch 209/400, Contrastive Loss: 3.1466\n",
      "Epoch 210/400, Contrastive Loss: 3.1202\n",
      "Epoch 211/400, Contrastive Loss: 3.1609\n",
      "Epoch 212/400, Contrastive Loss: 3.2354\n",
      "Epoch 213/400, Contrastive Loss: 3.3421\n",
      "Epoch 214/400, Contrastive Loss: 3.3118\n",
      "Epoch 215/400, Contrastive Loss: 3.0664\n",
      "Epoch 216/400, Contrastive Loss: 3.1101\n",
      "Epoch 217/400, Contrastive Loss: 3.2475\n",
      "Epoch 218/400, Contrastive Loss: 3.1385\n",
      "Epoch 219/400, Contrastive Loss: 3.1356\n",
      "Epoch 220/400, Contrastive Loss: 3.1791\n",
      "Epoch 221/400, Contrastive Loss: 3.1063\n",
      "Epoch 222/400, Contrastive Loss: 3.1044\n",
      "Epoch 223/400, Contrastive Loss: 3.1303\n",
      "Epoch 224/400, Contrastive Loss: 3.1203\n",
      "Epoch 225/400, Contrastive Loss: 3.2820\n",
      "Epoch 226/400, Contrastive Loss: 3.0597\n",
      "Epoch 227/400, Contrastive Loss: 3.1403\n",
      "Epoch 228/400, Contrastive Loss: 3.1232\n",
      "Epoch 229/400, Contrastive Loss: 3.0966\n",
      "Epoch 230/400, Contrastive Loss: 3.0648\n",
      "Epoch 231/400, Contrastive Loss: 3.2340\n",
      "Epoch 232/400, Contrastive Loss: 3.1057\n",
      "Epoch 233/400, Contrastive Loss: 3.1069\n",
      "Epoch 234/400, Contrastive Loss: 3.0597\n",
      "Epoch 235/400, Contrastive Loss: 3.0860\n",
      "Epoch 236/400, Contrastive Loss: 3.1735\n",
      "Epoch 237/400, Contrastive Loss: 3.1702\n",
      "Epoch 238/400, Contrastive Loss: 3.0142\n",
      "Epoch 239/400, Contrastive Loss: 3.0992\n",
      "Epoch 240/400, Contrastive Loss: 3.1959\n",
      "Epoch 241/400, Contrastive Loss: 3.0798\n",
      "Epoch 242/400, Contrastive Loss: 3.2054\n",
      "Epoch 243/400, Contrastive Loss: 3.0922\n",
      "Epoch 244/400, Contrastive Loss: 3.1173\n",
      "Epoch 245/400, Contrastive Loss: 2.9909\n",
      "Epoch 246/400, Contrastive Loss: 3.1134\n",
      "Epoch 247/400, Contrastive Loss: 3.1580\n",
      "Epoch 248/400, Contrastive Loss: 3.3100\n",
      "Epoch 249/400, Contrastive Loss: 3.1055\n",
      "Epoch 250/400, Contrastive Loss: 3.1530\n",
      "Epoch 251/400, Contrastive Loss: 3.2909\n",
      "Epoch 252/400, Contrastive Loss: 3.2639\n",
      "Epoch 253/400, Contrastive Loss: 3.3094\n",
      "Epoch 254/400, Contrastive Loss: 3.0118\n",
      "Epoch 255/400, Contrastive Loss: 3.1290\n",
      "Epoch 256/400, Contrastive Loss: 2.9750\n",
      "Epoch 257/400, Contrastive Loss: 3.2149\n",
      "Epoch 258/400, Contrastive Loss: 3.1152\n",
      "Epoch 259/400, Contrastive Loss: 3.3977\n",
      "Epoch 260/400, Contrastive Loss: 3.2739\n",
      "Epoch 261/400, Contrastive Loss: 3.0882\n",
      "Epoch 262/400, Contrastive Loss: 3.0539\n",
      "Epoch 263/400, Contrastive Loss: 3.1444\n",
      "Epoch 264/400, Contrastive Loss: 3.2092\n",
      "Epoch 265/400, Contrastive Loss: 3.2029\n",
      "Epoch 266/400, Contrastive Loss: 3.1496\n",
      "Epoch 267/400, Contrastive Loss: 3.1589\n",
      "Epoch 268/400, Contrastive Loss: 3.1935\n",
      "Epoch 269/400, Contrastive Loss: 3.1384\n",
      "Epoch 270/400, Contrastive Loss: 3.0703\n",
      "Epoch 271/400, Contrastive Loss: 3.2936\n",
      "Epoch 272/400, Contrastive Loss: 3.1056\n",
      "Epoch 273/400, Contrastive Loss: 3.1507\n",
      "Epoch 274/400, Contrastive Loss: 3.1703\n",
      "Epoch 275/400, Contrastive Loss: 3.2806\n",
      "Epoch 276/400, Contrastive Loss: 3.2152\n",
      "Epoch 277/400, Contrastive Loss: 3.0346\n",
      "Epoch 278/400, Contrastive Loss: 3.2542\n",
      "Epoch 279/400, Contrastive Loss: 3.1902\n",
      "Epoch 280/400, Contrastive Loss: 3.2228\n",
      "Epoch 281/400, Contrastive Loss: 3.1228\n",
      "Epoch 282/400, Contrastive Loss: 3.1135\n",
      "Epoch 283/400, Contrastive Loss: 3.1217\n",
      "Epoch 284/400, Contrastive Loss: 3.0858\n",
      "Epoch 285/400, Contrastive Loss: 3.2249\n",
      "Epoch 286/400, Contrastive Loss: 3.0711\n",
      "Epoch 287/400, Contrastive Loss: 3.1600\n",
      "Epoch 288/400, Contrastive Loss: 3.1741\n",
      "Epoch 289/400, Contrastive Loss: 3.1247\n",
      "Epoch 290/400, Contrastive Loss: 3.1660\n",
      "Epoch 291/400, Contrastive Loss: 3.2360\n",
      "Epoch 292/400, Contrastive Loss: 3.0731\n",
      "Epoch 293/400, Contrastive Loss: 3.4405\n",
      "Epoch 294/400, Contrastive Loss: 3.6505\n",
      "Epoch 295/400, Contrastive Loss: 3.2434\n",
      "Epoch 296/400, Contrastive Loss: 3.4412\n",
      "Epoch 297/400, Contrastive Loss: 3.6185\n",
      "Epoch 298/400, Contrastive Loss: 2.9983\n",
      "Epoch 299/400, Contrastive Loss: 3.2852\n",
      "Epoch 300/400, Contrastive Loss: 3.6004\n",
      "Epoch 301/400, Contrastive Loss: 3.2879\n",
      "Epoch 302/400, Contrastive Loss: 3.2234\n",
      "Epoch 303/400, Contrastive Loss: 3.3034\n",
      "Epoch 304/400, Contrastive Loss: 3.2944\n",
      "Epoch 305/400, Contrastive Loss: 3.1396\n",
      "Epoch 306/400, Contrastive Loss: 3.2354\n",
      "Epoch 307/400, Contrastive Loss: 3.2321\n",
      "Epoch 308/400, Contrastive Loss: 3.1838\n",
      "Epoch 309/400, Contrastive Loss: 3.2184\n",
      "Epoch 310/400, Contrastive Loss: 3.1295\n",
      "Epoch 311/400, Contrastive Loss: 3.1445\n",
      "Epoch 312/400, Contrastive Loss: 3.2059\n",
      "Epoch 313/400, Contrastive Loss: 3.3073\n",
      "Epoch 314/400, Contrastive Loss: 3.0685\n",
      "Epoch 315/400, Contrastive Loss: 3.2252\n",
      "Epoch 316/400, Contrastive Loss: 3.2187\n",
      "Epoch 317/400, Contrastive Loss: 3.2926\n",
      "Epoch 318/400, Contrastive Loss: 3.1430\n",
      "Epoch 319/400, Contrastive Loss: 3.1195\n",
      "Epoch 320/400, Contrastive Loss: 3.0854\n",
      "Epoch 321/400, Contrastive Loss: 3.0936\n",
      "Epoch 322/400, Contrastive Loss: 3.1386\n",
      "Epoch 323/400, Contrastive Loss: 3.1773\n",
      "Epoch 324/400, Contrastive Loss: 3.0278\n",
      "Epoch 325/400, Contrastive Loss: 3.1228\n",
      "Epoch 326/400, Contrastive Loss: 3.0759\n",
      "Epoch 327/400, Contrastive Loss: 3.0607\n",
      "Epoch 328/400, Contrastive Loss: 3.0603\n",
      "Epoch 329/400, Contrastive Loss: 2.9939\n",
      "Epoch 330/400, Contrastive Loss: 3.0531\n",
      "Epoch 331/400, Contrastive Loss: 3.0491\n",
      "Epoch 332/400, Contrastive Loss: 3.1000\n",
      "Epoch 333/400, Contrastive Loss: 3.0101\n",
      "Epoch 334/400, Contrastive Loss: 3.1284\n",
      "Epoch 335/400, Contrastive Loss: 3.0469\n",
      "Epoch 336/400, Contrastive Loss: 3.1295\n",
      "Epoch 337/400, Contrastive Loss: 3.1617\n",
      "Epoch 338/400, Contrastive Loss: 3.0298\n",
      "Epoch 339/400, Contrastive Loss: 3.1335\n",
      "Epoch 340/400, Contrastive Loss: 3.0400\n",
      "Epoch 341/400, Contrastive Loss: 2.9719\n",
      "Epoch 342/400, Contrastive Loss: 2.9617\n",
      "Epoch 343/400, Contrastive Loss: 2.9299\n",
      "Epoch 344/400, Contrastive Loss: 2.9477\n",
      "Epoch 345/400, Contrastive Loss: 3.0244\n",
      "Epoch 346/400, Contrastive Loss: 3.0789\n",
      "Epoch 347/400, Contrastive Loss: 2.8913\n",
      "Epoch 348/400, Contrastive Loss: 2.9862\n",
      "Epoch 349/400, Contrastive Loss: 3.3412\n",
      "Epoch 350/400, Contrastive Loss: 3.3223\n",
      "Epoch 351/400, Contrastive Loss: 3.2648\n",
      "Epoch 352/400, Contrastive Loss: 3.1720\n",
      "Epoch 353/400, Contrastive Loss: 3.0169\n",
      "Epoch 354/400, Contrastive Loss: 3.1619\n",
      "Epoch 355/400, Contrastive Loss: 3.0672\n",
      "Epoch 356/400, Contrastive Loss: 3.1183\n",
      "Epoch 357/400, Contrastive Loss: 3.0336\n",
      "Epoch 358/400, Contrastive Loss: 3.0615\n",
      "Epoch 359/400, Contrastive Loss: 3.2217\n",
      "Epoch 360/400, Contrastive Loss: 3.1508\n",
      "Epoch 361/400, Contrastive Loss: 2.9893\n",
      "Epoch 362/400, Contrastive Loss: 2.9500\n",
      "Epoch 363/400, Contrastive Loss: 3.2257\n",
      "Epoch 364/400, Contrastive Loss: 3.1494\n",
      "Epoch 365/400, Contrastive Loss: 3.0589\n",
      "Epoch 366/400, Contrastive Loss: 2.9987\n",
      "Epoch 367/400, Contrastive Loss: 3.1212\n",
      "Epoch 368/400, Contrastive Loss: 3.1205\n",
      "Epoch 369/400, Contrastive Loss: 3.1216\n",
      "Epoch 370/400, Contrastive Loss: 3.1077\n",
      "Epoch 371/400, Contrastive Loss: 3.0612\n",
      "Epoch 372/400, Contrastive Loss: 3.0852\n",
      "Epoch 373/400, Contrastive Loss: 3.0789\n",
      "Epoch 374/400, Contrastive Loss: 3.1791\n",
      "Epoch 375/400, Contrastive Loss: 3.2320\n",
      "Epoch 376/400, Contrastive Loss: 3.0858\n",
      "Epoch 377/400, Contrastive Loss: 3.1195\n",
      "Epoch 378/400, Contrastive Loss: 2.9983\n",
      "Epoch 379/400, Contrastive Loss: 3.0443\n",
      "Epoch 380/400, Contrastive Loss: 3.0168\n",
      "Epoch 381/400, Contrastive Loss: 3.0566\n",
      "Epoch 382/400, Contrastive Loss: 2.9083\n",
      "Epoch 383/400, Contrastive Loss: 2.9498\n",
      "Epoch 384/400, Contrastive Loss: 3.0014\n",
      "Epoch 385/400, Contrastive Loss: 3.0815\n",
      "Epoch 386/400, Contrastive Loss: 2.9405\n",
      "Epoch 387/400, Contrastive Loss: 2.9801\n",
      "Epoch 388/400, Contrastive Loss: 2.9599\n",
      "Epoch 389/400, Contrastive Loss: 2.9410\n",
      "Epoch 390/400, Contrastive Loss: 2.9611\n",
      "Epoch 391/400, Contrastive Loss: 3.1248\n",
      "Epoch 392/400, Contrastive Loss: 3.0774\n",
      "Epoch 393/400, Contrastive Loss: 3.0433\n",
      "Epoch 394/400, Contrastive Loss: 3.0222\n",
      "Epoch 395/400, Contrastive Loss: 2.9579\n",
      "Epoch 396/400, Contrastive Loss: 3.3309\n",
      "Epoch 397/400, Contrastive Loss: 3.3189\n",
      "Epoch 398/400, Contrastive Loss: 2.9595\n",
      "Epoch 399/400, Contrastive Loss: 2.9941\n",
      "Epoch 400/400, Contrastive Loss: 3.0646\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step\n",
      "Encoder Output - Mean: -0.0049408874 Std Dev: 0.12430094\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, LSTM, Dropout, Dense, BatchNormalization\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.optimizers import AdamW\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.keras.optimizers.schedules import ExponentialDecay\n",
    "\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D, GlobalAveragePooling1D\n",
    "\n",
    "\n",
    "scaler_raweeg = MinMaxScaler()\n",
    "scaler_features = MinMaxScaler()\n",
    "\n",
    "X_train_raweeg = scaler_raweeg.fit_transform(X_train_raweeg.reshape(-1, X_train_raweeg.shape[-1])).reshape(X_train_raweeg.shape)\n",
    "X_test_raweeg = scaler_raweeg.transform(X_test_raweeg.reshape(-1, X_test_raweeg.shape[-1])).reshape(X_test_raweeg.shape)\n",
    "\n",
    "X_train_features = scaler_features.fit_transform(X_train_features.reshape(-1, X_train_features.shape[-1])).reshape(X_train_features.shape)\n",
    "X_test_features = scaler_features.transform(X_test_features.reshape(-1, X_test_features.shape[-1])).reshape(X_test_features.shape)\n",
    "\n",
    "print(X_train_raweeg.shape, X_train_features.shape, Y_train.shape)\n",
    "print(X_test_raweeg.shape, X_test_features.shape, Y_test.shape)\n",
    "\n",
    "def add_noise(data, noise_level=0.05):\n",
    "    return data + noise_level * np.random.randn(*data.shape)\n",
    "\n",
    "X_train_raweeg = add_noise(X_train_raweeg, noise_level=0.001)\n",
    "X_train_features = add_noise(X_train_features, noise_level=0.01)\n",
    "\n",
    "\n",
    "# Encoder function with L2 normalization before output\n",
    "def create_cnn_encoder(input_shape):\n",
    "    inputs = Input(shape=input_shape)\n",
    "    x = Conv1D(16, kernel_size=3, activation='relu')(inputs)\n",
    "    x = MaxPooling1D(pool_size=2)(x)\n",
    "    x = Conv1D(32, kernel_size=3, activation='relu')(x)\n",
    "    x = GlobalAveragePooling1D()(x)\n",
    "    x = Dense(32, activation=None)(x)\n",
    "    model = Model(inputs=inputs, outputs=x)\n",
    "    return model\n",
    "\n",
    "\n",
    "def create_lstm_encoder(input_shape):\n",
    "    inputs = Input(shape=input_shape)\n",
    "    x = LSTM(32, return_sequences=False, kernel_regularizer=l2(1e-4))(inputs)\n",
    "    x = Dropout(0.5)(x)\n",
    "    x = Dense(32, activation=None, kernel_regularizer=l2(1e-4))(x)\n",
    "    model = Model(inputs=inputs, outputs=x)\n",
    "    return model\n",
    "\n",
    "\n",
    "encoder_raweeg = create_cnn_encoder(X_train_raweeg.shape[1:])\n",
    "encoder_features = create_cnn_encoder(X_train_features.shape[1:])\n",
    "\n",
    "\n",
    "def contrastive_loss(z_raweeg, z_features, temperature=0.2):\n",
    "    epsilon = 1e-6\n",
    "    z_raweeg = tf.math.l2_normalize(z_raweeg + epsilon, axis=1)\n",
    "    z_features = tf.math.l2_normalize(z_features + epsilon, axis=1)\n",
    "    logits = tf.matmul(z_raweeg, z_features, transpose_b=True) / temperature\n",
    "    batch_size = tf.shape(logits)[0]\n",
    "    labels = tf.range(batch_size)\n",
    "    loss = tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)\n",
    "    return tf.reduce_mean(loss)\n",
    "\n",
    "\n",
    "\n",
    "lr_schedule = ExponentialDecay(\n",
    "    initial_learning_rate=1e-2,\n",
    "    decay_steps=1000,\n",
    "    decay_rate=0.96,\n",
    "    staircase=True\n",
    ")\n",
    "\n",
    "optimizer = AdamW(learning_rate=lr_schedule, weight_decay=1e-3, clipnorm=0.6)\n",
    "epochs = 400\n",
    "batch_size = 64 \n",
    "\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    idx = np.random.permutation(len(X_train_raweeg))\n",
    "    X_raweeg_shuffled = X_train_raweeg[idx]\n",
    "    X_features_shuffled = X_train_features[idx]\n",
    "    losses = []\n",
    "\n",
    "    # print(len(X_train_raweeg) - batch_size + 1, batch_size)\n",
    "    for i in range(0, len(X_train_raweeg) - batch_size + 1, batch_size):  # Ensure full batches\n",
    "        x_r_batch = X_raweeg_shuffled[i:i+batch_size]\n",
    "        x_f_batch = X_features_shuffled[i:i+batch_size]\n",
    "        with tf.GradientTape() as tape:\n",
    "            z_r = encoder_raweeg(x_r_batch, training=True)\n",
    "            z_f = encoder_features(x_f_batch, training=True)\n",
    "            loss = contrastive_loss(z_r, z_f)\n",
    "\n",
    "        grads = tape.gradient(loss, encoder_raweeg.trainable_variables + encoder_features.trainable_variables)\n",
    "\n",
    "        # Check for NaN in gradients\n",
    "        if any(tf.math.reduce_any(tf.math.is_nan(g)) for g in grads if g is not None):\n",
    "            print(\"NaN detected in gradients! Stopping training.\")\n",
    "            break\n",
    "\n",
    "        optimizer.apply_gradients(zip(grads, encoder_raweeg.trainable_variables + encoder_features.trainable_variables))\n",
    "        losses.append(loss.numpy())\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{epochs}, Contrastive Loss: {np.mean(losses):.4f}\")\n",
    "\n",
    "# Check if embeddings are meaningful (should have variance)\n",
    "z_r_sample = encoder_raweeg.predict(X_train_raweeg[:10])\n",
    "print(\"Encoder Output - Mean:\", np.mean(z_r_sample), \"Std Dev:\", np.std(z_r_sample))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorow'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[39], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m accuracy_score, confusion_matrix, f1_score\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorow\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtf\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Model\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlayers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Input, LSTM, Dropout, Dense, BatchNormalization, Conv1D, MaxPooling1D, GlobalAveragePooling1D, Concatenate\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tensorow'"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import Concatenate\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, f1_score\n",
    "import numpy as np\n",
    "import tensorow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, LSTM, Dropout, Dense, BatchNormalization, Conv1D, MaxPooling1D, GlobalAveragePooling1D, Concatenate\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.optimizers import AdamW\n",
    "from tensorflow.keras.optimizers.schedules import ExponentialDecay\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, f1_score, confusion_matrix\n",
    "k = 5\n",
    "epochs = 400\n",
    "kf = StratifiedKFold(n_splits=k, shuffle=True, random_state=42)\n",
    "num_classes = Y_labels.shape[1]\n",
    "\n",
    "metrics_all = { 'loss': [], 'val_loss': [], 'accuracy': [], 'val_accuracy': [], 'auc': [], 'val_auc': [],\n",
    "                'precision': [], 'val_precision': [], 'recall': [], 'val_recall': [],\n",
    "                'f1': [], 'val_f1': [], 'specificity': [], 'val_specificity': [], 'confusion_matrix': []}\n",
    "\n",
    "metrics_per_class = { f'{metric}_class_{i}': [] for i in range(num_classes)\n",
    "                      for metric in ['precision', 'recall', 'f1', 'specificity',\n",
    "                                     'val_precision', 'val_recall', 'val_f1', 'val_specificity','val_acc'] }\n",
    "\n",
    "true_labels = []\n",
    "pred_labels = []\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(kf.split(X_train_raweeg, np.argmax(Y_train, axis=1))):\n",
    "    print(f\"\\n--------------- Fold {fold + 1}/{k}\")\n",
    "\n",
    "    X_train_raweeg_fold, X_val_raweeg_fold = X_train_raweeg[train_idx], X_train_raweeg[val_idx]\n",
    "    X_train_features_fold, X_val_features_fold = X_train_features[train_idx], X_train_features[val_idx]\n",
    "    Y_train_fold, Y_val_fold = Y_train[train_idx], Y_train[val_idx]\n",
    "\n",
    "    input_raweeg = Input(shape=(X_train_raweeg.shape[1], X_train_raweeg.shape[2]))\n",
    "    input_features = Input(shape=(X_train_features.shape[1], X_train_features.shape[2]))\n",
    "\n",
    "    z_raweeg = encoder_raweeg(input_raweeg)\n",
    "    z_features = encoder_features(input_features)\n",
    "\n",
    "    combined = Concatenate()([z_raweeg, z_features])\n",
    "    output = Dense(num_classes, activation='softmax')(combined)\n",
    "\n",
    "    classifier = Model(inputs=[input_raweeg, input_features], outputs=output)\n",
    "    classifier.compile(optimizer=AdamW(learning_rate=1e-2, weight_decay=1e-4),\n",
    "                       loss='categorical_crossentropy',\n",
    "                       metrics=['accuracy', tf.keras.metrics.AUC(name='auc'),\n",
    "                                tf.keras.metrics.Precision(name='precision'),\n",
    "                                tf.keras.metrics.Recall(name='recall'),\n",
    "                                tf.keras.metrics.SpecificityAtSensitivity(0.5, name='specificity')] +\n",
    "                                [tf.keras.metrics.Precision(name=f'precision_class_{i}', class_id=i) for i in range(num_classes)] +\n",
    "                                [tf.keras.metrics.Recall(name=f'recall_class_{i}', class_id=i) for i in range(num_classes)])\n",
    "\n",
    "\n",
    "    history = classifier.fit([X_train_raweeg_fold, X_train_features_fold], Y_train_fold,\n",
    "                             validation_data=([X_val_raweeg_fold, X_val_features_fold], Y_val_fold),\n",
    "                             epochs=epochs, batch_size=128, verbose=1,\n",
    "                             callbacks=[ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=1e-6)])\n",
    "\n",
    "\n",
    "\n",
    "    for metric in metrics_all.keys():\n",
    "        # print(metric)\n",
    "        if metric in history.history:\n",
    "            metrics_all[metric].append(history.history[metric])\n",
    "\n",
    "    y_pred = classifier.predict([X_val_raweeg_fold, X_val_features_fold])\n",
    "    y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "    y_true_classes = np.argmax(Y_val_fold, axis=1)\n",
    "\n",
    "    true_labels.extend(y_true_classes)\n",
    "    pred_labels.extend(y_pred_classes)\n",
    "\n",
    "    cm = confusion_matrix(y_true_classes, y_pred_classes)\n",
    "    cm_dict = {f\"row_{i}\": row.tolist() for i, row in enumerate(cm)}\n",
    "\n",
    "    metrics_all['confusion_matrix'] = cm_dict\n",
    "\n",
    "    for i in range(num_classes):\n",
    "            metrics_per_class[f'precision_class_{i}'].append(history.history[f'precision_class_{i}'])\n",
    "            metrics_per_class[f'recall_class_{i}'].append(history.history[f'recall_class_{i}'])\n",
    "            f1_class = f1_score(y_true_classes, y_pred_classes, average=None)[i]\n",
    "            metrics_per_class[f'f1_class_{i}'].append(f1_class)\n",
    "            mask = (y_true_classes == i)\n",
    "            metrics_per_class[f'val_acc_class_{i}'].append(accuracy_score(y_true_classes[mask], y_pred_classes[mask]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
      "\n",
      "Test Accuracy: 0.7000\n",
      "Test AUC: 0.7900\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, roc_auc_score, confusion_matrix\n",
    "\n",
    "y_pred_test = classifier.predict([X_test_raweeg, X_test_features])\n",
    "y_pred_classes_test = np.argmax(y_pred_test, axis=1)\n",
    "y_true_classes_test = np.argmax(Y_test, axis=1)\n",
    "\n",
    "test_accuracy = accuracy_score(y_true_classes_test, y_pred_classes_test)\n",
    "test_auc = roc_auc_score(Y_test, y_pred_test, multi_class='ovr')\n",
    "conf_matrix = confusion_matrix(y_true_classes_test, y_pred_classes_test)\n",
    "\n",
    "print(f\"\\nTest Accuracy: {test_accuracy:.4f}\")\n",
    "print(f\"Test AUC: {test_auc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CognitiveFatigue",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
