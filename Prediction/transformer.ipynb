{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import scipy.io as sio\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "import os\n",
    "import torch\n",
    "# import scipy.io as sio\n",
    "from torch.utils.data import Dataset\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "class MatFileDataset(Dataset):\n",
    "    def __init__(self, directory):\n",
    "        self.features = []  # To store 4-channel fractal features\n",
    "        self.signals = []   # To store 4-channel raw signals\n",
    "        self.labels = []    # To store labels\n",
    "        self.fractal_feature_length = None\n",
    "        self._load_data(directory)\n",
    "        self._validate_labels()\n",
    "\n",
    "    def _load_data(self, directory):\n",
    "        # Temporary storage for grouping channels\n",
    "        grouped_data = defaultdict(lambda: defaultdict(list))\n",
    "\n",
    "        # Step 1: Read all files and group by participant and window\n",
    "        for filename in os.listdir(directory):\n",
    "            if filename.endswith(\".mat\"):\n",
    "                filepath = os.path.join(directory, filename)\n",
    "                mat_data = sio.loadmat(filepath, struct_as_record=False, squeeze_me=True)\n",
    "\n",
    "                all_window_features = mat_data.get(\"all_window_features\")\n",
    "                if all_window_features is None:\n",
    "                    continue\n",
    "\n",
    "                # Extract the channel name from the file name\n",
    "                channel_name = filename.split('_')[-2]  # Assumes \"TP10\" or similar is always at the second last position\n",
    "\n",
    "                for participant_index, participant_data in enumerate(all_window_features):\n",
    "                    if participant_data is None:\n",
    "                        continue\n",
    "\n",
    "                    for window_index, window in enumerate(participant_data):\n",
    "                        # Extract labels first for filtering\n",
    "                        before_label = getattr(window, \"before_label\", None)\n",
    "                        after_label = getattr(window, \"after_label\", None)\n",
    "                        \n",
    "                        # Filter condition: both labels must exist and be <=3\n",
    "                        if (before_label is None or \n",
    "                            after_label is None or \n",
    "                            not (0 <= before_label <= 3) or \n",
    "                            not (0 <= after_label <= 3)):\n",
    "                            continue\n",
    "\n",
    "                        # Extract features and raw signals\n",
    "                        fractal_features = []\n",
    "                        raw_signals = []\n",
    "\n",
    "                        if hasattr(window, \"raw_window_signal\") and window.raw_window_signal is not None:\n",
    "                            raw_signals = window.raw_window_signal.flatten()\n",
    "                        if hasattr(window, \"Dq\") and window.Dq is not None:\n",
    "                            fractal_features = window.Dq.flatten()\n",
    "                        \n",
    "                        # Store data temporarily, grouped by participant and window\n",
    "                        grouped_data[(participant_index, window_index)][\"channels\"].append(channel_name)\n",
    "                        grouped_data[(participant_index, window_index)][\"fractal\"].append(torch.tensor(fractal_features, dtype=torch.float32))\n",
    "                        grouped_data[(participant_index, window_index)][\"signal\"].append(torch.tensor(raw_signals, dtype=torch.float32))\n",
    "                        grouped_data[(participant_index, window_index)][\"label\"] = (int(before_label), int(after_label))\n",
    "                        \n",
    "                        # Set fractal feature length from the first valid feature\n",
    "                        if self.fractal_feature_length is None and len(fractal_features) > 0:\n",
    "                            self.fractal_feature_length = len(fractal_features)\n",
    "\n",
    "        # Step 2: Combine data for all channels\n",
    "        for (participant_index, window_index), data in grouped_data.items():\n",
    "            channels = data[\"channels\"]\n",
    "            if len(channels) != 4:  # Ensure all 4 channels are present\n",
    "                continue\n",
    "            \n",
    "            # Stack fractal features and signals along a new dimension (channel dimension)\n",
    "            fractal_features = torch.stack(data[\"fractal\"], dim=0)  # Shape: (4, fractal_feature_length)\n",
    "            raw_signals = torch.stack(data[\"signal\"], dim=0)        # Shape: (4, signal_length)\n",
    "\n",
    "            self.features.append(fractal_features)\n",
    "            self.signals.append(raw_signals)\n",
    "            self.labels.append(data[\"label\"])\n",
    "\n",
    "    def _validate_labels(self):\n",
    "        \"\"\"Ensure all labels are valid integers 0-3\"\"\"\n",
    "        valid_before = all(0 <= lbl[0] <= 3 for lbl in self.labels)\n",
    "        valid_after = all(0 <= lbl[1] <= 3 for lbl in self.labels)\n",
    "        if not (valid_before and valid_after):\n",
    "            invalid = [\n",
    "                (i, lbl) for i, lbl in enumerate(self.labels)\n",
    "                if not (0 <= lbl[0] <= 3 and 0 <= lbl[1] <= 3)\n",
    "            ]\n",
    "            raise ValueError(f\"Invalid labels found at indices: {invalid[:10]}\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.features)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = {\n",
    "            'fractal': self.features[idx],  # Shape: (4, fractal_feature_length)\n",
    "            'signal': self.signals[idx],    # Shape: (4, signal_length)\n",
    "        }\n",
    "        y = {\n",
    "            \"before_label\": torch.tensor(self.labels[idx][0], dtype=torch.long),\n",
    "            \"after_label\": torch.tensor(self.labels[idx][1], dtype=torch.long),\n",
    "        }\n",
    "        return x, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 28908 valid samples\n"
     ]
    }
   ],
   "source": [
    "dataset = MatFileDataset(\"/Users/athenasaghi/VSProjects/CognitiveFatigueDetection/Prediction/\")\n",
    "print(f\"Loaded {len(dataset)} valid samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class EEGTransformerClassifier(nn.Module):\n",
    "    def __init__(self, input_channels, seq_length, num_classes, num_heads=4, d_model=64, num_layers=2, dim_feedforward=128, dropout=0.1):\n",
    "        super(EEGTransformerClassifier, self).__init__()\n",
    "        self.embedding = nn.Linear(input_channels, d_model)  # Embed input_channels to d_model\n",
    "        self.positional_encoding = nn.Parameter(torch.zeros(1, seq_length, d_model))\n",
    "        self.num_classes = num_classes\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=d_model, \n",
    "            nhead=num_heads, \n",
    "            dim_feedforward=dim_feedforward, \n",
    "            dropout=dropout\n",
    "        )\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(seq_length * d_model, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(128, num_classes * 2)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.permute(0, 2, 1)  # Shape: [batch_size, seq_length, input_channels]\n",
    "        x = self.embedding(x)  # Shape: [batch_size, seq_length, d_model]\n",
    "        x = x + self.positional_encoding[:, :x.size(1), :]\n",
    "        x = x.permute(1, 0, 2)  # Shape: [seq_length, batch_size, d_model]\n",
    "        x = self.transformer_encoder(x)\n",
    "        x = x.permute(1, 0, 2)  # Shape: [batch_size, seq_length, d_model]\n",
    "        x = x.flatten(1)  # Shape: [batch_size, seq_length * d_model]\n",
    "        x = self.fc(x)\n",
    "        x = x.view(-1, 2, self.num_classes)  # Shape: [batch_size, 2, num_classes]\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "# batch_size = 32\n",
    "# input_channels = 4\n",
    "# seq_length = 41\n",
    "# num_classes = 4\n",
    "# learning_rate = 0.001\n",
    "\n",
    "# num_epochs = 10\n",
    "\n",
    "# criterion = nn.CrossEntropyLoss()\n",
    "# model = EEGTransformerClassifier(input_channels, seq_length, num_classes)\n",
    "# optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# for epoch in range(num_epochs):\n",
    "\n",
    "#     for batch in dataloader:\n",
    "#         x, y = batch\n",
    "#         # print(x['fractal'].shape, x['signal'].shape)\n",
    "#         # print(y['before_label'].shape, y['after_label'].shape)\n",
    "#         if np.isinf(x['fractal']).any():\n",
    "#             max_finite = torch.max(x['fractal'][~torch.isinf(x['fractal'])])\n",
    "#             x['fractal'][torch.isinf(x['fractal'])] = max_finite\n",
    "#         if np.isinf(x['fractal']).any():\n",
    "#             raise ValueError(\"Infinite values found in fractal features\")\n",
    "#         if np.isnan(x['fractal']).any():\n",
    "#             mean_finite = torch.mean(x['fractal'][~torch.isnan(x['fractal'])])\n",
    "#             x['fractal'][torch.isnan(x['fractal'])] = mean_finite\n",
    "        \n",
    "\n",
    "#         inputs = x['fractal']\n",
    "#         outputs = model(inputs)\n",
    "#         loss_1 = criterion(outputs[:, 0, :], y['before_label'])\n",
    "#         loss_2 = criterion(outputs[:, 1, :], y['after_label'])\n",
    "#         loss = loss_1 + loss_2\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "\n",
    "#         with torch.no_grad():\n",
    "#             pred_1 = outputs[:, 0, :].argmax(dim=1)\n",
    "#             pred_2 = outputs[:, 1, :].argmax(dim=1)\n",
    "#             acc_1 = (pred_1 == y['before_label']).float().mean().item()\n",
    "#             acc_2 = (pred_2 == y['after_label']).float().mean().item()\n",
    "\n",
    "#     print(f\"Epoch {epoch + 1}/{num_epochs}, Loss_1: {loss_1.item():.4f}, Loss_2: {loss_2.item():.4f}, Total Loss: {loss.item():.4f}, Acc_1: {acc_1:.4f}, Acc_2: {acc_2:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torch.utils.data import DataLoader, random_split\n",
    "# import numpy as np\n",
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# import torch.optim as optim\n",
    "\n",
    "# # Dataset split\n",
    "# train_size = int(0.7 * len(dataset))\n",
    "# val_size = int(0.15 * len(dataset))\n",
    "# test_size = len(dataset) - train_size - val_size\n",
    "# train_dataset, val_dataset, test_dataset = random_split(dataset, [train_size, val_size, test_size])\n",
    "\n",
    "# batch_size = 16\n",
    "\n",
    "\n",
    "# train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "# val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "# test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# input_channels = 4\n",
    "# seq_length = 41\n",
    "# num_classes = 4\n",
    "# learning_rate = 0.01\n",
    "\n",
    "# num_epochs = 10\n",
    "\n",
    "# criterion = nn.CrossEntropyLoss()\n",
    "# model = EEGTransformerClassifier(input_channels, seq_length, num_classes)\n",
    "# optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# def process_batch(loader, model, criterion, optimizer=None, train_mode=False):\n",
    "#     total_loss = 0\n",
    "#     total_acc_1 = 0\n",
    "#     total_acc_2 = 0\n",
    "#     if train_mode:\n",
    "#         model.train()\n",
    "#     else:\n",
    "#         model.eval()\n",
    "\n",
    "#     with torch.set_grad_enabled(train_mode):\n",
    "#         for batch in loader:\n",
    "#             x, y = batch\n",
    "#             if np.isinf(x['fractal']).any():\n",
    "#                 max_finite = torch.max(x['fractal'][~torch.isinf(x['fractal'])])\n",
    "#                 x['fractal'][torch.isinf(x['fractal'])] = max_finite\n",
    "#             if np.isnan(x['fractal']).any():\n",
    "#                 mean_finite = torch.mean(x['fractal'][~torch.isnan(x['fractal'])])\n",
    "#                 x['fractal'][torch.isnan(x['fractal'])] = mean_finite\n",
    "\n",
    "#             inputs = x['fractal']\n",
    "#             outputs = model(inputs)\n",
    "#             loss_1 = criterion(outputs[:, 0, :], y['before_label'])\n",
    "#             loss_2 = criterion(outputs[:, 1, :], y['after_label'])\n",
    "#             loss = loss_1 + loss_2\n",
    "\n",
    "#             if train_mode:\n",
    "#                 optimizer.zero_grad()\n",
    "#                 loss.backward()\n",
    "#                 optimizer.step()\n",
    "\n",
    "#             total_loss += loss.item()\n",
    "#             pred_1 = outputs[:, 0, :].argmax(dim=1)\n",
    "#             pred_2 = outputs[:, 1, :].argmax(dim=1)\n",
    "#             total_acc_1 += (pred_1 == y['before_label']).float().mean().item()\n",
    "#             total_acc_2 += (pred_2 == y['after_label']).float().mean().item()\n",
    "\n",
    "#     avg_loss = total_loss / len(loader)\n",
    "#     avg_acc_1 = total_acc_1 / len(loader)\n",
    "#     avg_acc_2 = total_acc_2 / len(loader)\n",
    "\n",
    "#     return avg_loss, avg_acc_1, avg_acc_2\n",
    "\n",
    "# for epoch in range(num_epochs):\n",
    "#     train_loss, train_acc_1, train_acc_2 = process_batch(train_loader, model, criterion, optimizer, train_mode=True)\n",
    "#     val_loss, val_acc_1, val_acc_2 = process_batch(val_loader, model, criterion, train_mode=False)\n",
    "\n",
    "#     print(f\"Epoch {epoch + 1}/{num_epochs}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}, Train Acc_1: {train_acc_1:.4f}, Train Acc_2: {train_acc_2:.4f}, Val Acc_1: {val_acc_1:.4f}, Val Acc_2: {val_acc_2:.4f}\")\n",
    "\n",
    "# test_loss, test_acc_1, test_acc_2 = process_batch(test_loader, model, criterion, train_mode=False)\n",
    "\n",
    "# print(f\"Test Loss: {test_loss:.4f}, Test Acc_1: {test_acc_1:.4f}, Test Acc_2: {test_acc_2:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.9862, Val Loss: 0.9859, Train Acc: 0.6297, Val Acc: 0.6264\n",
      "Epoch 2/10, Train Loss: 0.9733, Val Loss: 0.9722, Train Acc: 0.6317, Val Acc: 0.6264\n",
      "Epoch 3/10, Train Loss: 0.9710, Val Loss: 0.9655, Train Acc: 0.6317, Val Acc: 0.6264\n",
      "Epoch 4/10, Train Loss: 0.9703, Val Loss: 0.9695, Train Acc: 0.6317, Val Acc: 0.6264\n",
      "Epoch 5/10, Train Loss: 0.9689, Val Loss: 0.9653, Train Acc: 0.6316, Val Acc: 0.6264\n",
      "Epoch 6/10, Train Loss: 0.9698, Val Loss: 0.9647, Train Acc: 0.6315, Val Acc: 0.6264\n",
      "Epoch 7/10, Train Loss: 0.9675, Val Loss: 0.9688, Train Acc: 0.6315, Val Acc: 0.6264\n",
      "Epoch 8/10, Train Loss: 0.9663, Val Loss: 0.9681, Train Acc: 0.6318, Val Acc: 0.6264\n",
      "Epoch 9/10, Train Loss: 0.9659, Val Loss: 0.9610, Train Acc: 0.6314, Val Acc: 0.6264\n",
      "Epoch 10/10, Train Loss: 0.9641, Val Loss: 0.9665, Train Acc: 0.6319, Val Acc: 0.6264\n",
      "Test Loss: 0.9515, Test Acc: 0.6404\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader, random_split\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Dataset split\n",
    "train_size = int(0.7 * len(dataset))\n",
    "val_size = int(0.15 * len(dataset))\n",
    "test_size = len(dataset) - train_size - val_size\n",
    "train_dataset, val_dataset, test_dataset = random_split(dataset, [train_size, val_size, test_size])\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "\n",
    "input_channels = 4\n",
    "seq_length = 41\n",
    "num_classes = 4\n",
    "learning_rate = 0.001\n",
    "\n",
    "num_epochs = 10\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "model = EEGTransformerClassifier(input_channels, seq_length, num_classes)\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "def process_batch(loader, model, criterion, optimizer=None, train_mode=False):\n",
    "    total_loss = 0\n",
    "    total_acc = 0\n",
    "    if train_mode:\n",
    "        model.train()\n",
    "    else:\n",
    "        model.eval()\n",
    "\n",
    "    with torch.set_grad_enabled(train_mode):\n",
    "        for batch in loader:\n",
    "            x, y = batch\n",
    "            label = y['before_label']\n",
    "            if np.isinf(x['fractal']).any():\n",
    "                max_finite = torch.max(x['fractal'][~torch.isinf(x['fractal'])])\n",
    "                x['fractal'][torch.isinf(x['fractal'])] = max_finite\n",
    "            if np.isnan(x['fractal']).any():\n",
    "                mean_finite = torch.mean(x['fractal'][~torch.isnan(x['fractal'])])\n",
    "                x['fractal'][torch.isnan(x['fractal'])] = mean_finite\n",
    "\n",
    "            inputs = x['fractal']\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs[:, 0, :], label)\n",
    "\n",
    "            if train_mode:\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            preds = outputs[:, 0, :].argmax(dim=1)\n",
    "            total_acc += (preds == label).float().mean().item()\n",
    "\n",
    "    avg_loss = total_loss / len(loader)\n",
    "    avg_acc = total_acc / len(loader)\n",
    "\n",
    "    return avg_loss, avg_acc\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss, train_acc = process_batch(train_loader, model, criterion, optimizer, train_mode=True)\n",
    "    val_loss, val_acc = process_batch(val_loader, model, criterion, train_mode=False)\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}, Train Acc: {train_acc:.4f}, Val Acc: {val_acc:.4f}\")\n",
    "\n",
    "test_loss, test_acc = process_batch(test_loader, model, criterion, train_mode=False)\n",
    "\n",
    "print(f\"Test Loss: {test_loss:.4f}, Test Acc: {test_acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CognitiveFatigue",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
